{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruben-28/VE_Training/blob/main/VE_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TSlcDyyiY9JK",
        "outputId": "074e1899-18cb-4f9d-99b2-03f06fb51bb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 31 19:26:17 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install \"tensorflow==2.19.0\" \"numpy==2.0.2\" \"scipy==1.14.1\" \"scikit-learn==1.6.1\"\n",
        "!pip -q install -U opencv-python mediapipe tqdm matplotlib\n"
      ],
      "metadata": {
        "id": "suf2TI92aiQM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3646b4e8-8816-4da4-efa1-25075732de43"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m140.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "id": "q3QRie2IasdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf, numpy as np, scipy, sklearn\n",
        "import mediapipe as mp, cv2\n",
        "\n",
        "print(\"TF:\", tf.__version__)\n",
        "print(\"NumPy:\", np.__version__)\n",
        "print(\"SciPy:\", scipy.__version__)\n",
        "print(\"Sklearn:\", sklearn.__version__)\n",
        "print(\"MediaPipe:\", mp.__version__)\n",
        "print(\"OpenCV:\", cv2.__version__)\n",
        "print(\"GPU:\", tf.config.list_physical_devices(\"GPU\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "waDgaRYCavIq",
        "outputId": "c267195f-147b-4cc1-dd5f-0f5583338097"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF: 2.19.0\n",
            "NumPy: 2.0.2\n",
            "SciPy: 1.14.1\n",
            "Sklearn: 1.6.1\n",
            "MediaPipe: 0.10.31\n",
            "OpenCV: 4.12.0\n",
            "GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p /content/data/IMAGES_ALL\n",
        "unzip -q /content/IMAGES_ALL.zip -d /content/data/IMAGES_ALL\n",
        "\n",
        "# Corrige le double dossier si besoin\n",
        "if [ -d \"/content/data/IMAGES_ALL/IMAGES_ALL\" ]; then\n",
        "  mv /content/data/IMAGES_ALL/IMAGES_ALL/* /content/data/IMAGES_ALL/\n",
        "  rmdir /content/data/IMAGES_ALL/IMAGES_ALL\n",
        "fi\n",
        "\n",
        "echo \"Root:\"\n",
        "ls /content/data/IMAGES_ALL | head\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sXmIi00ha1F0",
        "outputId": "6f8a2ba0-ed06-47c1-e275-17a858e61b79"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root:\n",
            "FER2013\n",
            "RAFDB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile build_dataset.py\n",
        "from pathlib import Path\n",
        "import random, shutil, re\n",
        "from collections import defaultdict\n",
        "\n",
        "SRC = Path(\"/content/data/IMAGES_ALL\")\n",
        "DST = Path(\"/content/data/DATASET_FINAL\")\n",
        "\n",
        "CLASSES = [\"angry\",\"disgust\",\"fear\",\"happy\",\"neutral\",\"sad\",\"surprise\"]\n",
        "ALIASES = {\n",
        "    \"angry\":   [\"angry\", \"anger\", \"ang\", \"mad\"],\n",
        "    \"disgust\": [\"disgust\", \"disgusted\", \"disg\"],\n",
        "    \"fear\":    [\"fear\", \"fearful\", \"scared\", \"afr\"],\n",
        "    \"happy\":   [\"happy\", \"happiness\", \"smile\", \"joy\"],\n",
        "    \"neutral\": [\"neutral\", \"neu\", \"normal\"],\n",
        "    \"sad\":     [\"sad\", \"sadness\", \"unhappy\"],\n",
        "    \"surprise\":[\"surprise\", \"surprised\", \"surpr\"]\n",
        "}\n",
        "IMG_EXTS = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\"}\n",
        "SEED = 42\n",
        "\n",
        "def norm(s: str) -> str:\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "def label_from_text(text: str):\n",
        "    t = norm(text)\n",
        "    for cls, words in ALIASES.items():\n",
        "        for w in words:\n",
        "            if re.search(rf\"\\b{re.escape(w)}\\b\", t):\n",
        "                return cls\n",
        "    return None\n",
        "\n",
        "def guess_label(path: Path):\n",
        "    lab = label_from_text(path.name)\n",
        "    if lab:\n",
        "        return lab\n",
        "    parts = []\n",
        "    for p in path.parents:\n",
        "        parts.append(p.name)\n",
        "        if p == SRC:\n",
        "            break\n",
        "    return label_from_text(\" \".join(parts))\n",
        "\n",
        "def ensure_structure():\n",
        "    for split in [\"train\",\"val\",\"test\"]:\n",
        "        for c in CLASSES:\n",
        "            (DST/split/c).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def split_items(items):\n",
        "    random.seed(SEED)\n",
        "    random.shuffle(items)\n",
        "    n = len(items)\n",
        "    tr = int(n*0.7)\n",
        "    va = int(n*0.15)\n",
        "    return {\n",
        "        \"train\": items[:tr],\n",
        "        \"val\": items[tr:tr+va],\n",
        "        \"test\": items[tr+va:]\n",
        "    }\n",
        "\n",
        "def safe_copy(src: Path, dst: Path):\n",
        "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if dst.exists():\n",
        "        dst = dst.with_name(dst.stem + f\"__dup{random.randint(0,999999)}\" + dst.suffix)\n",
        "    shutil.copy2(src, dst)\n",
        "\n",
        "def main():\n",
        "    ensure_structure()\n",
        "\n",
        "    all_imgs = []\n",
        "    for p in SRC.rglob(\"*\"):\n",
        "        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
        "            lab = guess_label(p)\n",
        "            if lab in CLASSES:\n",
        "                all_imgs.append((p, lab))\n",
        "\n",
        "    print(\"Labellisées:\", len(all_imgs))\n",
        "    if not all_imgs:\n",
        "        print(\"Aucune image labellisée -> structure différente.\")\n",
        "        return\n",
        "\n",
        "    by_class = defaultdict(list)\n",
        "    for p, lab in all_imgs:\n",
        "        by_class[lab].append(p)\n",
        "\n",
        "    for lab, imgs in by_class.items():\n",
        "        splits = split_items(imgs)\n",
        "        for split, files in splits.items():\n",
        "            for f in files:\n",
        "                safe_copy(f, DST/split/lab/f.name)\n",
        "\n",
        "    print(\"Done ->\", DST)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jdYlvsHna8EK",
        "outputId": "f4bf1650-a648-47a8-da4a-3edc56b5cc45"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing build_dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python build_dataset.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5NRFeJEYa9a0",
        "outputId": "87599e6f-ee3a-4ae8-ce17-98c9c4b7b5ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labellisées: 77579\n",
            "Done -> /content/data/DATASET_FINAL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "for c in angry disgust fear happy neutral sad surprise; do\n",
        "  echo \"$c train: $(ls /content/data/DATASET_FINAL/train/$c | wc -l)\"\n",
        "done\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Hcc9rakRbFA7",
        "outputId": "2702acce-77a6-4cf8-dd99-423027ee3dc5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "angry train: 7636\n",
            "disgust train: 4552\n",
            "fear train: 7753\n",
            "happy train: 10461\n",
            "neutral train: 8507\n",
            "sad train: 8423\n",
            "surprise train: 6970\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p /content/models\n",
        "wget -q -O /content/models/blaze_face_short_range.tflite \\\n",
        "  \"https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/latest/blaze_face_short_range.tflite\"\n",
        "ls -lh /content/models/blaze_face_short_range.tflite\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lm9rURogbG-d",
        "outputId": "4b606952-60ee-4592-bb97-7b2096c87b92"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 225K Apr 26  2023 /content/models/blaze_face_short_range.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile crop_faces_tasks.py\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "SRC_ROOT = Path(\"/content/data/DATASET_FINAL\")\n",
        "DST_ROOT = Path(\"/content/data/DATASET_CROPPED\")\n",
        "MODEL_PATH = \"/content/models/blaze_face_short_range.tflite\"\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "MARGIN = 0.25\n",
        "MIN_CONF = 0.55\n",
        "SPLITS = [\"train\",\"val\",\"test\"]\n",
        "IMG_EXTS = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\"}\n",
        "\n",
        "def clamp(v, lo, hi): return max(lo, min(hi, v))\n",
        "\n",
        "def ensure_dirs():\n",
        "    for split in SPLITS:\n",
        "        sdir = SRC_ROOT/split\n",
        "        if not sdir.exists(): continue\n",
        "        for cls in sdir.iterdir():\n",
        "            if cls.is_dir():\n",
        "                (DST_ROOT/split/cls.name).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def crop_with_box(img_bgr, x1, y1, bw, bh):\n",
        "    h, w = img_bgr.shape[:2]\n",
        "    mx, my = int(bw*MARGIN), int(bh*MARGIN)\n",
        "    x1 = clamp(x1-mx, 0, w-1)\n",
        "    y1 = clamp(y1-my, 0, h-1)\n",
        "    x2 = clamp(x1+bw+2*mx, 1, w)\n",
        "    y2 = clamp(y1+bh+2*my, 1, h)\n",
        "    crop = img_bgr[y1:y2, x1:x2]\n",
        "    if crop.size == 0: return None\n",
        "    return cv2.resize(crop, IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "def main():\n",
        "    ensure_dirs()\n",
        "\n",
        "    BaseOptions = mp.tasks.BaseOptions\n",
        "    FaceDetector = mp.tasks.vision.FaceDetector\n",
        "    FaceDetectorOptions = mp.tasks.vision.FaceDetectorOptions\n",
        "    RunningMode = mp.tasks.vision.RunningMode\n",
        "\n",
        "    options = FaceDetectorOptions(\n",
        "        base_options=BaseOptions(model_asset_path=MODEL_PATH),\n",
        "        running_mode=RunningMode.IMAGE,\n",
        "        min_detection_confidence=MIN_CONF\n",
        "    )\n",
        "\n",
        "    total=kept=failed=0\n",
        "\n",
        "    with FaceDetector.create_from_options(options) as detector:\n",
        "        for split in SPLITS:\n",
        "            sdir = SRC_ROOT/split\n",
        "            if not sdir.exists(): continue\n",
        "\n",
        "            for cls_dir in sorted([d for d in sdir.iterdir() if d.is_dir()]):\n",
        "                out_dir = DST_ROOT/split/cls_dir.name\n",
        "                imgs=[]\n",
        "                for ext in IMG_EXTS:\n",
        "                    imgs += list(cls_dir.rglob(f\"*{ext}\"))\n",
        "\n",
        "                for p in tqdm(imgs, desc=f\"{split}/{cls_dir.name}\", leave=False):\n",
        "                    total += 1\n",
        "                    img = cv2.imread(str(p))\n",
        "                    if img is None:\n",
        "                        failed += 1\n",
        "                        continue\n",
        "\n",
        "                    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n",
        "\n",
        "                    res = detector.detect(mp_image)\n",
        "                    if not res.detections:\n",
        "                        failed += 1\n",
        "                        continue\n",
        "\n",
        "                    best = max(res.detections, key=lambda d: d.categories[0].score)\n",
        "                    score = best.categories[0].score\n",
        "                    if score < MIN_CONF:\n",
        "                        failed += 1\n",
        "                        continue\n",
        "\n",
        "                    bb = best.bounding_box\n",
        "                    crop = crop_with_box(img, bb.origin_x, bb.origin_y, bb.width, bb.height)\n",
        "                    if crop is None:\n",
        "                        failed += 1\n",
        "                        continue\n",
        "\n",
        "                    out_path = out_dir / p.name\n",
        "                    if out_path.exists():\n",
        "                        out_path = out_dir / f\"{p.stem}__{random.randint(0,999999)}{p.suffix}\"\n",
        "                    cv2.imwrite(str(out_path), crop)\n",
        "                    kept += 1\n",
        "\n",
        "    print(\"Total images:\", total)\n",
        "    print(\"Cropped kept:\", kept)\n",
        "    print(\"Failed/no face:\", failed)\n",
        "    print(\"Output:\", DST_ROOT)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9sEgJf0ObKSS",
        "outputId": "d3d0ae8c-56c7-4b08-c436-532161c64ca5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing crop_faces_tasks.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python crop_faces_tasks.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HnZLHNhTbPr2",
        "outputId": "ac202c11-0df7-4e7a-c91f-2d0cae4f2568"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-31 19:28:16.242159: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1767209296.263484    1642 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1767209296.270153    1642 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1767209296.285061    1642 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767209296.285092    1642 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767209296.285096    1642 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767209296.285101    1642 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "W0000 00:00:1767209302.345496    1697 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
            "Total images: 77579\n",
            "Cropped kept: 62651\n",
            "Failed/no face: 14928\n",
            "Output: /content/data/DATASET_CROPPED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "DATA_ROOT = Path(\"/content/data/DATASET_CROPPED\")\n",
        "TRAIN_DIR = DATA_ROOT/\"train\"\n",
        "VAL_DIR   = DATA_ROOT/\"val\"\n",
        "TEST_DIR  = DATA_ROOT/\"test\"\n",
        "\n",
        "IMG_SIZE = (224,224)\n",
        "BATCH_SIZE = 32          # safe T4 + RAM\n",
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "    TRAIN_DIR, image_size=IMG_SIZE, batch_size=BATCH_SIZE, seed=SEED, label_mode=\"int\", shuffle=True\n",
        ")\n",
        "val_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "    VAL_DIR, image_size=IMG_SIZE, batch_size=BATCH_SIZE, seed=SEED, label_mode=\"int\", shuffle=False\n",
        ")\n",
        "test_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "    TEST_DIR, image_size=IMG_SIZE, batch_size=BATCH_SIZE, seed=SEED, label_mode=\"int\", shuffle=False\n",
        ")\n",
        "\n",
        "class_names = train_raw.class_names\n",
        "num_classes = len(class_names)\n",
        "print(\"Classes:\", class_names)\n",
        "\n",
        "# ❗ pas de cache() RAM\n",
        "train_ds = train_raw.prefetch(AUTOTUNE)\n",
        "val_ds   = val_raw.prefetch(AUTOTUNE)\n",
        "test_ds  = test_raw.prefetch(AUTOTUNE)\n",
        "\n",
        "# class weights\n",
        "y_train=[]\n",
        "for _, y in train_raw:\n",
        "    y_train.extend(y.numpy().tolist())\n",
        "y_train = np.array(y_train, dtype=np.int64)\n",
        "\n",
        "weights = compute_class_weight(\"balanced\", classes=np.arange(num_classes), y=y_train)\n",
        "class_weight = {i: float(w) for i, w in enumerate(weights)}\n",
        "print(\"class_weight:\", class_weight)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LbQtwZkBcj5f",
        "outputId": "f6482a9c-b26e-4b77-f547-f5e0bc3a150a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 43904 files belonging to 7 classes.\n",
            "Found 9434 files belonging to 7 classes.\n",
            "Found 9313 files belonging to 7 classes.\n",
            "Classes: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
            "class_weight: {0: 1.1087148665370337, 1: 1.9011821764170962, 2: 1.000957548675391, 3: 0.7063858542628675, 4: 0.8734159587801142, 5: 0.9316696375519905, 6: 1.0648556876061122}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.03),\n",
        "    tf.keras.layers.RandomZoom(0.08),\n",
        "    tf.keras.layers.RandomContrast(0.10),\n",
        "], name=\"augment\")\n",
        "\n",
        "base = tf.keras.applications.EfficientNetV2B0(\n",
        "    include_top=False, weights=\"imagenet\", input_shape=(224,224,3)\n",
        ")\n",
        "base.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(224,224,3))\n",
        "x = data_augmentation(inputs)\n",
        "x = tf.keras.applications.efficientnet_v2.preprocess_input(x)\n",
        "x = base(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.35)(x)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"best_cropped.keras\", monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True, verbose=1),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1),\n",
        "]\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "IwMEwcUzc6-A",
        "outputId": "0fd73ac0-630c-4a5b-8e36-feb201744f93"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n",
            "\u001b[1m24274472/24274472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ augment (\u001b[38;5;33mSequential\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetv2-b0 (\u001b[38;5;33mFunctional\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m5,919,312\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │         \u001b[38;5;34m8,967\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ augment (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetv2-b0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,919,312</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,967</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,928,279\u001b[0m (22.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,928,279</span> (22.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,967\u001b[0m (35.03 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,967</span> (35.03 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m5,919,312\u001b[0m (22.58 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,919,312</span> (22.58 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history1 = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=15,\n",
        "    class_weight=class_weight,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2dY7asErc9RS",
        "outputId": "0fa7e76e-d382-4376-8e4f-e13bfc666375"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.3148 - loss: 1.7348\n",
            "Epoch 1: val_accuracy improved from -inf to 0.46163, saving model to best_cropped.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 79ms/step - accuracy: 0.3149 - loss: 1.7347 - val_accuracy: 0.4616 - val_loss: 1.4594 - learning_rate: 0.0010\n",
            "Epoch 2/15\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.4190 - loss: 1.5297\n",
            "Epoch 2: val_accuracy improved from 0.46163 to 0.48166, saving model to best_cropped.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 72ms/step - accuracy: 0.4190 - loss: 1.5297 - val_accuracy: 0.4817 - val_loss: 1.3969 - learning_rate: 0.0010\n",
            "Epoch 3/15\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4360 - loss: 1.4973\n",
            "Epoch 3: val_accuracy improved from 0.48166 to 0.48940, saving model to best_cropped.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 71ms/step - accuracy: 0.4360 - loss: 1.4973 - val_accuracy: 0.4894 - val_loss: 1.3764 - learning_rate: 0.0010\n",
            "Epoch 4/15\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4351 - loss: 1.4879\n",
            "Epoch 4: val_accuracy improved from 0.48940 to 0.49597, saving model to best_cropped.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 72ms/step - accuracy: 0.4351 - loss: 1.4879 - val_accuracy: 0.4960 - val_loss: 1.3614 - learning_rate: 0.0010\n",
            "Epoch 5/15\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.4431 - loss: 1.4844\n",
            "Epoch 5: val_accuracy did not improve from 0.49597\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 71ms/step - accuracy: 0.4431 - loss: 1.4844 - val_accuracy: 0.4951 - val_loss: 1.3726 - learning_rate: 0.0010\n",
            "Epoch 6/15\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4390 - loss: 1.4796\n",
            "Epoch 6: val_accuracy did not improve from 0.49597\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 70ms/step - accuracy: 0.4391 - loss: 1.4796 - val_accuracy: 0.4884 - val_loss: 1.3726 - learning_rate: 0.0010\n",
            "Epoch 7/15\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4414 - loss: 1.4768\n",
            "Epoch 7: val_accuracy improved from 0.49597 to 0.50286, saving model to best_cropped.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 71ms/step - accuracy: 0.4414 - loss: 1.4768 - val_accuracy: 0.5029 - val_loss: 1.3440 - learning_rate: 5.0000e-04\n",
            "Epoch 8/15\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4494 - loss: 1.4645\n",
            "Epoch 8: val_accuracy did not improve from 0.50286\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 70ms/step - accuracy: 0.4494 - loss: 1.4645 - val_accuracy: 0.5008 - val_loss: 1.3447 - learning_rate: 5.0000e-04\n",
            "Epoch 9/15\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4445 - loss: 1.4652\n",
            "Epoch 9: val_accuracy improved from 0.50286 to 0.50551, saving model to best_cropped.keras\n",
            "\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 71ms/step - accuracy: 0.4445 - loss: 1.4652 - val_accuracy: 0.5055 - val_loss: 1.3453 - learning_rate: 5.0000e-04\n",
            "Epoch 10/15\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4469 - loss: 1.4621\n",
            "Epoch 10: val_accuracy improved from 0.50551 to 0.50657, saving model to best_cropped.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 71ms/step - accuracy: 0.4469 - loss: 1.4621 - val_accuracy: 0.5066 - val_loss: 1.3437 - learning_rate: 2.5000e-04\n",
            "Epoch 11/15\n",
            "\u001b[1m1371/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4520 - loss: 1.4511\n",
            "Epoch 11: val_accuracy did not improve from 0.50657\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 70ms/step - accuracy: 0.4520 - loss: 1.4511 - val_accuracy: 0.5064 - val_loss: 1.3424 - learning_rate: 2.5000e-04\n",
            "Epoch 12/15\n",
            "\u001b[1m1371/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4499 - loss: 1.4547\n",
            "Epoch 12: val_accuracy did not improve from 0.50657\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 70ms/step - accuracy: 0.4499 - loss: 1.4547 - val_accuracy: 0.5036 - val_loss: 1.3430 - learning_rate: 2.5000e-04\n",
            "Epoch 13/15\n",
            "\u001b[1m1371/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.4512 - loss: 1.4542\n",
            "Epoch 13: val_accuracy did not improve from 0.50657\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 71ms/step - accuracy: 0.4512 - loss: 1.4542 - val_accuracy: 0.5054 - val_loss: 1.3407 - learning_rate: 2.5000e-04\n",
            "Epoch 14/15\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4530 - loss: 1.4520\n",
            "Epoch 14: val_accuracy improved from 0.50657 to 0.50943, saving model to best_cropped.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 71ms/step - accuracy: 0.4530 - loss: 1.4520 - val_accuracy: 0.5094 - val_loss: 1.3398 - learning_rate: 2.5000e-04\n",
            "Epoch 15/15\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4475 - loss: 1.4574\n",
            "Epoch 15: val_accuracy improved from 0.50943 to 0.51060, saving model to best_cropped.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 71ms/step - accuracy: 0.4475 - loss: 1.4574 - val_accuracy: 0.5106 - val_loss: 1.3376 - learning_rate: 2.5000e-04\n",
            "Restoring model weights from the end of the best epoch: 15.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.load_model(\"best_cropped.keras\")\n",
        "\n",
        "# retrouver backbone (EfficientNetV2B0)\n",
        "backbone = None\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, tf.keras.Model) and \"efficientnet\" in layer.name.lower():\n",
        "        backbone = layer\n",
        "        break\n",
        "print(\"Backbone:\", backbone.name)\n",
        "\n",
        "# Déverrouille le backbone\n",
        "backbone.trainable = True\n",
        "\n",
        "# Déverrouiller plus que 50 couches (pour sortir de l’underfitting)\n",
        "FINE_TUNE_LAYERS = 120\n",
        "for l in backbone.layers[:-FINE_TUNE_LAYERS]:\n",
        "    l.trainable = False\n",
        "\n",
        "# LR plus bas\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(3e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "callbacks_ft = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"best_cropped_ft.keras\", monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=4, restore_best_weights=True, verbose=1),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1),\n",
        "]\n",
        "\n",
        "history_ft = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10,\n",
        "    class_weight=class_weight,\n",
        "    callbacks=callbacks_ft\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4_kJir3qvORB",
        "outputId": "ef3ef5cb-7eb2-4ea3-8b30-b0738676ddd1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backbone: efficientnetv2-b0\n",
            "Epoch 1/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.3695 - loss: 1.6577\n",
            "Epoch 1: val_accuracy improved from -inf to 0.56561, saving model to best_cropped_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 124ms/step - accuracy: 0.3695 - loss: 1.6576 - val_accuracy: 0.5656 - val_loss: 1.1725 - learning_rate: 3.0000e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.5395 - loss: 1.2360\n",
            "Epoch 2: val_accuracy improved from 0.56561 to 0.62434, saving model to best_cropped_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 122ms/step - accuracy: 0.5395 - loss: 1.2360 - val_accuracy: 0.6243 - val_loss: 1.0211 - learning_rate: 3.0000e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.6081 - loss: 1.0542\n",
            "Epoch 3: val_accuracy improved from 0.62434 to 0.66144, saving model to best_cropped_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 122ms/step - accuracy: 0.6081 - loss: 1.0542 - val_accuracy: 0.6614 - val_loss: 0.9369 - learning_rate: 3.0000e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.6469 - loss: 0.9487\n",
            "Epoch 4: val_accuracy improved from 0.66144 to 0.68274, saving model to best_cropped_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 126ms/step - accuracy: 0.6469 - loss: 0.9486 - val_accuracy: 0.6827 - val_loss: 0.8758 - learning_rate: 3.0000e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.6811 - loss: 0.8599\n",
            "Epoch 5: val_accuracy improved from 0.68274 to 0.69928, saving model to best_cropped_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 122ms/step - accuracy: 0.6811 - loss: 0.8599 - val_accuracy: 0.6993 - val_loss: 0.8284 - learning_rate: 3.0000e-05\n",
            "Epoch 6/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.7008 - loss: 0.8023\n",
            "Epoch 6: val_accuracy improved from 0.69928 to 0.71423, saving model to best_cropped_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 122ms/step - accuracy: 0.7008 - loss: 0.8023 - val_accuracy: 0.7142 - val_loss: 0.7925 - learning_rate: 3.0000e-05\n",
            "Epoch 7/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.7218 - loss: 0.7439\n",
            "Epoch 7: val_accuracy improved from 0.71423 to 0.72006, saving model to best_cropped_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 123ms/step - accuracy: 0.7218 - loss: 0.7439 - val_accuracy: 0.7201 - val_loss: 0.7736 - learning_rate: 3.0000e-05\n",
            "Epoch 8/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.7378 - loss: 0.6969\n",
            "Epoch 8: val_accuracy improved from 0.72006 to 0.72822, saving model to best_cropped_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 123ms/step - accuracy: 0.7378 - loss: 0.6968 - val_accuracy: 0.7282 - val_loss: 0.7500 - learning_rate: 3.0000e-05\n",
            "Epoch 9/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.7489 - loss: 0.6643\n",
            "Epoch 9: val_accuracy improved from 0.72822 to 0.73564, saving model to best_cropped_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 122ms/step - accuracy: 0.7489 - loss: 0.6643 - val_accuracy: 0.7356 - val_loss: 0.7344 - learning_rate: 3.0000e-05\n",
            "Epoch 10/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.7626 - loss: 0.6338\n",
            "Epoch 10: val_accuracy improved from 0.73564 to 0.74136, saving model to best_cropped_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 123ms/step - accuracy: 0.7626 - loss: 0.6338 - val_accuracy: 0.7414 - val_loss: 0.7172 - learning_rate: 3.0000e-05\n",
            "Restoring model weights from the end of the best epoch: 10.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.load_model(\"best_cropped_ft.keras\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_ds, verbose=1)\n",
        "print(\"Test accuracy:\", test_acc)\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for x, y in test_ds:\n",
        "    p = model.predict(x, verbose=0)\n",
        "    y_true.extend(y.numpy().tolist())\n",
        "    y_pred.extend(np.argmax(p, axis=1).tolist())\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.imshow(cm)\n",
        "plt.title(\"Confusion Matrix (CROPPED - Fine-tuned)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.xticks(range(num_classes), class_names, rotation=45, ha=\"right\")\n",
        "plt.yticks(range(num_classes), class_names)\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "MTzC8CPE19oQ",
        "outputId": "2ff2555e-bbf6-41ac-f5d9-a9787c75bc7a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 63ms/step - accuracy: 0.7242 - loss: 0.7654\n",
            "Test accuracy: 0.7401481866836548\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry     0.7135    0.7348    0.7240      1169\n",
            "     disgust     0.8588    0.8575    0.8582       702\n",
            "        fear     0.7857    0.6621    0.7186      1323\n",
            "       happy     0.8699    0.8198    0.8441      1892\n",
            "     neutral     0.6080    0.6867    0.6450      1529\n",
            "         sad     0.5880    0.6459    0.6156      1443\n",
            "    surprise     0.8539    0.8151    0.8341      1255\n",
            "\n",
            "    accuracy                         0.7401      9313\n",
            "   macro avg     0.7540    0.7460    0.7485      9313\n",
            "weighted avg     0.7486    0.7401    0.7428      9313\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.load_model(\"best_cropped_ft.keras\")\n",
        "model.save(\"/content/emotion_model_ft_v1.keras\")\n",
        "print(\"✅ Saved:\", \"/content/emotion_model_ft_v1.keras\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3EXgN4xH3EFy",
        "outputId": "63e0d0bf-0197-4d65-d5a3-c5e9d6813a55"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: /content/emotion_model_ft_v1.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.export(\"/content/emotion_model_ft_v1_savedmodel\")\n",
        "print(\"✅ Exported:\", \"/content/emotion_model_ft_v1_savedmodel\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4oioebaF4QfF",
        "outputId": "746eb8d9-6b02-46c8-fbca-86cd66c83695"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/content/emotion_model_ft_v1_savedmodel'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_layer_1')]\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 7), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134820094952272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134820094967632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134822525854416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134820094952080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256930192: TensorSpec(shape=(1, 1, 1, 3), dtype=tf.float16, name=None)\n",
            "  134818256929808: TensorSpec(shape=(1, 1, 1, 3), dtype=tf.float16, name=None)\n",
            "  134818256932880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256929616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256930576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256930000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256931152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256932112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256931536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256930384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256932688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256936912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256934224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256930768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256936336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256935568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256937296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256935760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256931920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256937488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256937872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256938448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256937680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256936720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256938256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256938832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256939408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256938640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256937104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256939216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256939792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256940368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256940560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256939024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256939984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256940944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256941520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256940176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256938064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256941328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256941904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256942480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256941712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256932304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256942288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256942864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256943440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256942672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256941136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256943248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256943824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256944400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256944592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256943056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256929424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256940752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256942096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256944976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256944016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007270608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256944208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818256944784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007269648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007271184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007270032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007270416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007270992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007269840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007271760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007270800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007272528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007270224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007271952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007272336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007272912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007273488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007272720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007272144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007273296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007273872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007274448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007273680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007274832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007271568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007274256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007274640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007273104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007275408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007274064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007276176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007276368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007275792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007275024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007276752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007277328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007275984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007275600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007277136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007277712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007278288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007277520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007278672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007276560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007278096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007278480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007276944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007279248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007277904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007280016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007280208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007279632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007278864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007280592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007281168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007279824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007279440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007280976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007281552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007282128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007281360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007282512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007280400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007281936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007282320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007280784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007283088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007281744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007283856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007279056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007283280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007283664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007284240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007284816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007284048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007284624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007285008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007271376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007284432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007285392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007282896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007283472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007282704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007285200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006073808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006074960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818007285584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006074384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006075344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006074192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006075152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006075728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006076304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006074000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006074576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006076112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006076688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006077264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006076496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006077648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006075536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006077072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006077456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006075920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006078224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006076880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006078992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006079184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006078608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006077840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006079568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006080144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006078800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006078416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006079952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006080528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006081104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006080336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006081488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006079376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006080912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006081296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006079760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006082064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006080720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006082832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006083024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006082448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006081680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006083408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006083984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006082640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006082256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006083792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006084368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006084944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006084176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006085328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006083216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006084752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006085136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006083600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006085904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006084560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006086672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006086864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006086288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006085520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006087248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006087824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006086480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006086096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006087632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006088208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006088784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006088016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006087056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006085712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006087440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006073616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006088592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006089168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006088400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006088976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006089360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004927888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004928080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818006089552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004926928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004926544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004926736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004927312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004928656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004929232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004928464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004929616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004927504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004929040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004929424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004928272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004930192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004928848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004930960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004931152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004930576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004929808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004931536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004932112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004930768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004930384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004931920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004932496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004933072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004932304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004933456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004931344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004932880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004933264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004931728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004934032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004932688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004934800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004934992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004934416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004933648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004935376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004935952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004934608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004934224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004935760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004936336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004936912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004936144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004937296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004935184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004936720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004937104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004935568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004937872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004936528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004938640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004938832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004938256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004937488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004939216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004939792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004938448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004938064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004939600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004940176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004940752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004939984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004941136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004939024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004940560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004940944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004939408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004941712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004940368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004942480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004942672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004941520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004927120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004941328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004942288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004941904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003845968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003845584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818004942096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003846928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003845392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003847312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003846160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003846736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003847120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003845776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003847888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003846544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003848656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003848848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003848272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003847504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003849232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003849808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003848464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003848080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003849616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003850192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003850768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003850000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003851152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003849040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003850576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003850960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003849424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003851728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003850384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003852496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003852688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003852112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003851344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003853072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003853648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003852304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003851920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003853456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003854032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003854608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003853840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003854992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003852880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003854416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003854800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003853264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003855568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003854224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003856336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003856528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003855952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003855184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003856912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003857488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003845200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134818003856720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "✅ Exported: /content/emotion_model_ft_v1_savedmodel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/emotion_model_ft_v1.keras\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "940S0g-o4X-o",
        "outputId": "c47d9464-507b-4553-8edb-69d7ef4dd7e7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7be88d52-aabd-4003-9351-08041a1843c8\", \"emotion_model_ft_v1.keras\", 95565338)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.load_model(\"best_cropped_ft.keras\")\n",
        "\n",
        "# retrouver backbone\n",
        "backbone = None\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, tf.keras.Model) and \"efficientnet\" in layer.name.lower():\n",
        "        backbone = layer\n",
        "        break\n",
        "print(\"Backbone:\", backbone.name)\n",
        "\n",
        "backbone.trainable = True\n",
        "\n",
        "# Déverrouille plus qu'avant\n",
        "FINE_TUNE_LAYERS = 220\n",
        "for l in backbone.layers[:-FINE_TUNE_LAYERS]:\n",
        "    l.trainable = False\n",
        "\n",
        "# LR plus bas (important)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "callbacks_ft2 = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"best_cropped_ft2.keras\", monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3, restore_best_weights=True, verbose=1),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, verbose=1),\n",
        "]\n",
        "\n",
        "history_ft2 = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=8,\n",
        "    class_weight=class_weight,\n",
        "    callbacks=callbacks_ft2\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "isRTtfNE4v2Y",
        "outputId": "b4b6273a-1d07-49b7-c067-1b5876040de5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backbone: efficientnetv2-b0\n",
            "Epoch 1/8\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.7725 - loss: 0.6071\n",
            "Epoch 1: val_accuracy improved from -inf to 0.74327, saving model to best_cropped_ft2.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 126ms/step - accuracy: 0.7725 - loss: 0.6070 - val_accuracy: 0.7433 - val_loss: 0.7133 - learning_rate: 1.0000e-05\n",
            "Epoch 2/8\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.7772 - loss: 0.5883\n",
            "Epoch 2: val_accuracy improved from 0.74327 to 0.74772, saving model to best_cropped_ft2.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 123ms/step - accuracy: 0.7772 - loss: 0.5883 - val_accuracy: 0.7477 - val_loss: 0.7037 - learning_rate: 1.0000e-05\n",
            "Epoch 3/8\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.7828 - loss: 0.5760\n",
            "Epoch 3: val_accuracy did not improve from 0.74772\n",
            "\n",
            "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 124ms/step - accuracy: 0.7828 - loss: 0.5760 - val_accuracy: 0.7473 - val_loss: 0.7038 - learning_rate: 1.0000e-05\n",
            "Epoch 4/8\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.7879 - loss: 0.5651\n",
            "Epoch 4: val_accuracy improved from 0.74772 to 0.74804, saving model to best_cropped_ft2.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 124ms/step - accuracy: 0.7879 - loss: 0.5651 - val_accuracy: 0.7480 - val_loss: 0.6983 - learning_rate: 5.0000e-06\n",
            "Epoch 5/8\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.7881 - loss: 0.5655\n",
            "Epoch 5: val_accuracy improved from 0.74804 to 0.74878, saving model to best_cropped_ft2.keras\n",
            "\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 123ms/step - accuracy: 0.7881 - loss: 0.5655 - val_accuracy: 0.7488 - val_loss: 0.6998 - learning_rate: 5.0000e-06\n",
            "Epoch 6/8\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.7912 - loss: 0.5560\n",
            "Epoch 6: val_accuracy did not improve from 0.74878\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 121ms/step - accuracy: 0.7912 - loss: 0.5560 - val_accuracy: 0.7475 - val_loss: 0.7011 - learning_rate: 2.5000e-06\n",
            "Epoch 7/8\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.7881 - loss: 0.5596\n",
            "Epoch 7: val_accuracy did not improve from 0.74878\n",
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 123ms/step - accuracy: 0.7881 - loss: 0.5596 - val_accuracy: 0.7477 - val_loss: 0.7030 - learning_rate: 1.2500e-06\n",
            "Epoch 8/8\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.7868 - loss: 0.5557\n",
            "Epoch 8: val_accuracy improved from 0.74878 to 0.74973, saving model to best_cropped_ft2.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 123ms/step - accuracy: 0.7868 - loss: 0.5557 - val_accuracy: 0.7497 - val_loss: 0.6972 - learning_rate: 6.2500e-07\n",
            "Restoring model weights from the end of the best epoch: 8.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model2 = tf.keras.models.load_model(\"best_cropped_ft2.keras\")\n",
        "\n",
        "test_loss, test_acc = model2.evaluate(test_ds, verbose=1)\n",
        "print(\"✅ Test accuracy FT2:\", test_acc)\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for x, y in test_ds:\n",
        "    p = model2.predict(x, verbose=0)\n",
        "    y_true.extend(y.numpy().tolist())\n",
        "    y_pred.extend(np.argmax(p, axis=1).tolist())\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4PVXxel6-RZw",
        "outputId": "a78034d2-40a0-4679-88ff-767084c6f925"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 50ms/step - accuracy: 0.7267 - loss: 0.7479\n",
            "✅ Test accuracy FT2: 0.7466981410980225\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry     0.7270    0.7357    0.7313      1169\n",
            "     disgust     0.8750    0.8575    0.8662       702\n",
            "        fear     0.7692    0.6803    0.7220      1323\n",
            "       happy     0.8845    0.8177    0.8498      1892\n",
            "     neutral     0.6337    0.6756    0.6540      1529\n",
            "         sad     0.5775    0.6764    0.6230      1443\n",
            "    surprise     0.8612    0.8255    0.8430      1255\n",
            "\n",
            "    accuracy                         0.7467      9313\n",
            "   macro avg     0.7612    0.7527    0.7556      9313\n",
            "weighted avg     0.7558    0.7467    0.7498      9313\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "m = tf.keras.models.load_model(\"best_cropped_ft2.keras\")\n",
        "m.save(\"/content/emotion_model_ft2.keras\")\n",
        "print(\"✅ Saved: /content/emotion_model_ft2.keras\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-rD_6JX1_eFO",
        "outputId": "f03d0510-cc72-4969-89cc-97e282ae5403"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: /content/emotion_model_ft2.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def to_one_hot(x, y):\n",
        "    y = tf.one_hot(y, depth=num_classes)\n",
        "    return x, y\n",
        "\n",
        "train_ds_oh = train_ds.map(to_one_hot, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds_oh   = val_ds.map(to_one_hot, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds_oh  = test_ds.map(to_one_hot, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds_oh = train_ds_oh.prefetch(tf.data.AUTOTUNE)\n",
        "val_ds_oh   = val_ds_oh.prefetch(tf.data.AUTOTUNE)\n",
        "test_ds_oh  = test_ds_oh.prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "uWBtJb6t_ila"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.load_model(\"best_cropped_ft2.keras\")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-6),  # très petit LR\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.06),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "callbacks_ft3 = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"best_cropped_ft3.keras\", monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=2, restore_best_weights=True, verbose=1),\n",
        "]\n",
        "\n",
        "history_ft3 = model.fit(\n",
        "    train_ds_oh,\n",
        "    validation_data=val_ds_oh,\n",
        "    epochs=4,\n",
        "    class_weight=class_weight,\n",
        "    callbacks=callbacks_ft3\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "A2ZMrVG0At2R",
        "outputId": "39315be9-2c34-496f-884d-0d112b312566"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.7914 - loss: 0.8563\n",
            "Epoch 1: val_accuracy improved from -inf to 0.75101, saving model to best_cropped_ft3.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 126ms/step - accuracy: 0.7914 - loss: 0.8563 - val_accuracy: 0.7510 - val_loss: 0.9447\n",
            "Epoch 2/4\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.7907 - loss: 0.8394\n",
            "Epoch 2: val_accuracy did not improve from 0.75101\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 121ms/step - accuracy: 0.7907 - loss: 0.8394 - val_accuracy: 0.7486 - val_loss: 0.9414\n",
            "Epoch 3/4\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.7912 - loss: 0.8242\n",
            "Epoch 3: val_accuracy did not improve from 0.75101\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 122ms/step - accuracy: 0.7912 - loss: 0.8242 - val_accuracy: 0.7456 - val_loss: 0.9392\n",
            "Epoch 3: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model3 = tf.keras.models.load_model(\"best_cropped_ft3.keras\")\n",
        "\n",
        "test_loss, test_acc = model3.evaluate(test_ds_oh, verbose=1)\n",
        "print(\"✅ Test accuracy FT3:\", test_acc)\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for x, y in test_ds:\n",
        "    p = model3.predict(x, verbose=0)\n",
        "    y_true.extend(y.numpy().tolist())\n",
        "    y_pred.extend(tf.argmax(p, axis=1).numpy().tolist())\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_uS5kvPwDfDN",
        "outputId": "dab2a2fd-0376-4da0-b9e2-f350fc58c866"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 52ms/step - accuracy: 0.7240 - loss: 0.9980\n",
            "✅ Test accuracy FT3: 0.7485235929489136\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry     0.7298    0.7322    0.7310      1169\n",
            "     disgust     0.8951    0.8390    0.8662       702\n",
            "        fear     0.7822    0.6757    0.7251      1323\n",
            "       happy     0.8809    0.8251    0.8521      1892\n",
            "     neutral     0.6279    0.6841    0.6548      1529\n",
            "         sad     0.5791    0.6826    0.6266      1443\n",
            "    surprise     0.8667    0.8287    0.8473      1255\n",
            "\n",
            "    accuracy                         0.7485      9313\n",
            "   macro avg     0.7659    0.7525    0.7576      9313\n",
            "weighted avg     0.7587    0.7485    0.7519      9313\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.03),\n",
        "    tf.keras.layers.RandomZoom(0.08),\n",
        "    tf.keras.layers.RandomContrast(0.10),\n",
        "], name=\"augment\")\n",
        "\n",
        "base = tf.keras.applications.EfficientNetV2S(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(224,224,3)\n",
        ")\n",
        "base.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(224,224,3))\n",
        "x = data_augmentation(inputs)\n",
        "x = tf.keras.applications.efficientnet_v2.preprocess_input(x)\n",
        "x = base(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.35)(x)\n",
        "outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n",
        "\n",
        "modelS = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "modelS.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "callbacksS1 = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"best_v2s.keras\", monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=4, restore_best_weights=True, verbose=1),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1),\n",
        "]\n",
        "\n",
        "modelS.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "1rsDrS8sHSmB",
        "outputId": "1855d87b-132e-4b4d-c5c7-083eb256453e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-s_notop.h5\n",
            "\u001b[1m82420632/82420632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_35\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_35\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ augment (\u001b[38;5;33mSequential\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetv2-s (\u001b[38;5;33mFunctional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │    \u001b[38;5;34m20,331,360\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │         \u001b[38;5;34m8,967\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ augment (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ efficientnetv2-s (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">20,331,360</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,967</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,340,327\u001b[0m (77.59 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,340,327</span> (77.59 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,967\u001b[0m (35.03 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,967</span> (35.03 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m20,331,360\u001b[0m (77.56 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,331,360</span> (77.56 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "histS1 = modelS.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10,\n",
        "    class_weight=class_weight,\n",
        "    callbacks=callbacksS1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0Xauz_j7HbcU",
        "outputId": "3fe86d02-07aa-4ed3-ded5-1f0c716bc1cf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.3452 - loss: 1.7032\n",
            "Epoch 1: val_accuracy improved from -inf to 0.45696, saving model to best_v2s.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 161ms/step - accuracy: 0.3453 - loss: 1.7031 - val_accuracy: 0.4570 - val_loss: 1.4640 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.4260 - loss: 1.5225\n",
            "Epoch 2: val_accuracy improved from 0.45696 to 0.46937, saving model to best_v2s.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 156ms/step - accuracy: 0.4260 - loss: 1.5225 - val_accuracy: 0.4694 - val_loss: 1.4237 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.4361 - loss: 1.4998\n",
            "Epoch 3: val_accuracy improved from 0.46937 to 0.48378, saving model to best_v2s.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 150ms/step - accuracy: 0.4361 - loss: 1.4998 - val_accuracy: 0.4838 - val_loss: 1.3882 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.4370 - loss: 1.4974\n",
            "Epoch 4: val_accuracy improved from 0.48378 to 0.48707, saving model to best_v2s.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 150ms/step - accuracy: 0.4370 - loss: 1.4974 - val_accuracy: 0.4871 - val_loss: 1.3849 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.4452 - loss: 1.4903\n",
            "Epoch 5: val_accuracy improved from 0.48707 to 0.49131, saving model to best_v2s.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 151ms/step - accuracy: 0.4452 - loss: 1.4902 - val_accuracy: 0.4913 - val_loss: 1.3881 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.4468 - loss: 1.4809\n",
            "Epoch 6: val_accuracy did not improve from 0.49131\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 149ms/step - accuracy: 0.4468 - loss: 1.4809 - val_accuracy: 0.4900 - val_loss: 1.3692 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.4433 - loss: 1.4921\n",
            "Epoch 7: val_accuracy did not improve from 0.49131\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 149ms/step - accuracy: 0.4433 - loss: 1.4921 - val_accuracy: 0.4901 - val_loss: 1.3772 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.4421 - loss: 1.4863\n",
            "Epoch 8: val_accuracy improved from 0.49131 to 0.49597, saving model to best_v2s.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 151ms/step - accuracy: 0.4421 - loss: 1.4863 - val_accuracy: 0.4960 - val_loss: 1.3597 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.4454 - loss: 1.4820\n",
            "Epoch 9: val_accuracy improved from 0.49597 to 0.49714, saving model to best_v2s.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 151ms/step - accuracy: 0.4454 - loss: 1.4820 - val_accuracy: 0.4971 - val_loss: 1.3651 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.4459 - loss: 1.4833\n",
            "Epoch 10: val_accuracy did not improve from 0.49714\n",
            "\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 156ms/step - accuracy: 0.4459 - loss: 1.4833 - val_accuracy: 0.4953 - val_loss: 1.3758 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 9.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "modelS = tf.keras.models.load_model(\"best_v2s.keras\")\n",
        "\n",
        "# retrouver backbone\n",
        "backbone = None\n",
        "for layer in modelS.layers:\n",
        "    if isinstance(layer, tf.keras.Model) and \"efficientnet\" in layer.name.lower():\n",
        "        backbone = layer\n",
        "        break\n",
        "print(\"Backbone:\", backbone.name)\n",
        "\n",
        "backbone.trainable = True\n",
        "\n",
        "# déverrouille les dernières couches\n",
        "FINE_TUNE_LAYERS = 160\n",
        "for l in backbone.layers[:-FINE_TUNE_LAYERS]:\n",
        "    l.trainable = False\n",
        "\n",
        "modelS.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(2e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "callbacksS2 = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\"best_v2s_ft.keras\", monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=4, restore_best_weights=True, verbose=1),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, verbose=1),\n",
        "]\n",
        "\n",
        "histS2 = modelS.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10,\n",
        "    class_weight=class_weight,\n",
        "    callbacks=callbacksS2\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "im3plXxgPsuB",
        "outputId": "bde8b7d6-d31e-425f-a900-ad21a79f7989"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backbone: efficientnetv2-s\n",
            "Epoch 1/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.4205 - loss: 1.5412\n",
            "Epoch 1: val_accuracy improved from -inf to 0.58003, saving model to best_v2s_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 237ms/step - accuracy: 0.4205 - loss: 1.5412 - val_accuracy: 0.5800 - val_loss: 1.1320 - learning_rate: 2.0000e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.5496 - loss: 1.2140\n",
            "Epoch 2: val_accuracy improved from 0.58003 to 0.63165, saving model to best_v2s_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 229ms/step - accuracy: 0.5496 - loss: 1.2140 - val_accuracy: 0.6317 - val_loss: 0.9965 - learning_rate: 2.0000e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.6079 - loss: 1.0619\n",
            "Epoch 3: val_accuracy improved from 0.63165 to 0.67013, saving model to best_v2s_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 228ms/step - accuracy: 0.6079 - loss: 1.0619 - val_accuracy: 0.6701 - val_loss: 0.9015 - learning_rate: 2.0000e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.6439 - loss: 0.9605\n",
            "Epoch 4: val_accuracy improved from 0.67013 to 0.69568, saving model to best_v2s_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 231ms/step - accuracy: 0.6439 - loss: 0.9605 - val_accuracy: 0.6957 - val_loss: 0.8396 - learning_rate: 2.0000e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.6686 - loss: 0.8849\n",
            "Epoch 5: val_accuracy improved from 0.69568 to 0.71285, saving model to best_v2s_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 231ms/step - accuracy: 0.6686 - loss: 0.8849 - val_accuracy: 0.7128 - val_loss: 0.7907 - learning_rate: 2.0000e-05\n",
            "Epoch 6/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.6910 - loss: 0.8172\n",
            "Epoch 6: val_accuracy improved from 0.71285 to 0.72578, saving model to best_v2s_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 230ms/step - accuracy: 0.6910 - loss: 0.8172 - val_accuracy: 0.7258 - val_loss: 0.7633 - learning_rate: 2.0000e-05\n",
            "Epoch 7/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.7115 - loss: 0.7742\n",
            "Epoch 7: val_accuracy improved from 0.72578 to 0.73256, saving model to best_v2s_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 236ms/step - accuracy: 0.7115 - loss: 0.7742 - val_accuracy: 0.7326 - val_loss: 0.7392 - learning_rate: 2.0000e-05\n",
            "Epoch 8/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.7275 - loss: 0.7241\n",
            "Epoch 8: val_accuracy improved from 0.73256 to 0.73808, saving model to best_v2s_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 233ms/step - accuracy: 0.7275 - loss: 0.7241 - val_accuracy: 0.7381 - val_loss: 0.7259 - learning_rate: 2.0000e-05\n",
            "Epoch 9/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.7428 - loss: 0.6860\n",
            "Epoch 9: val_accuracy improved from 0.73808 to 0.74687, saving model to best_v2s_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 237ms/step - accuracy: 0.7428 - loss: 0.6860 - val_accuracy: 0.7469 - val_loss: 0.7065 - learning_rate: 2.0000e-05\n",
            "Epoch 10/10\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.7540 - loss: 0.6509\n",
            "Epoch 10: val_accuracy improved from 0.74687 to 0.75228, saving model to best_v2s_ft.keras\n",
            "\u001b[1m1372/1372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 241ms/step - accuracy: 0.7540 - loss: 0.6509 - val_accuracy: 0.7523 - val_loss: 0.6958 - learning_rate: 2.0000e-05\n",
            "Restoring model weights from the end of the best epoch: 10.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "bestS = tf.keras.models.load_model(\"best_v2s_ft.keras\")\n",
        "\n",
        "test_loss, test_acc = bestS.evaluate(test_ds, verbose=1)\n",
        "print(\"✅ Test accuracy V2S:\", test_acc)\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "for x, y in test_ds:\n",
        "    p = bestS.predict(x, verbose=0)\n",
        "    y_true.extend(y.numpy().tolist())\n",
        "    y_pred.extend(np.argmax(p, axis=1).tolist())\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kFCULIlhluSN",
        "outputId": "fc448575-b243-4588-8480-d670176f75e7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m292/292\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 124ms/step - accuracy: 0.7123 - loss: 0.8047\n",
            "✅ Test accuracy V2S: 0.7468055486679077\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry     0.7316    0.6903    0.7104      1169\n",
            "     disgust     0.8300    0.8903    0.8591       702\n",
            "        fear     0.7573    0.6629    0.7070      1323\n",
            "       happy     0.8555    0.8451    0.8503      1892\n",
            "     neutral     0.6296    0.7070    0.6661      1529\n",
            "         sad     0.6505    0.6126    0.6310      1443\n",
            "    surprise     0.7991    0.8622    0.8294      1255\n",
            "\n",
            "    accuracy                         0.7468      9313\n",
            "   macro avg     0.7505    0.7529    0.7505      9313\n",
            "weighted avg     0.7476    0.7468    0.7460      9313\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "bestS = tf.keras.models.load_model(\"best_v2s_ft.keras\")\n",
        "bestS.save(\"/content/emotion_faces_v2s_final.keras\")\n",
        "print(\"✅ Modèle final sauvegardé\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8P1VIYwFm7NH",
        "outputId": "50cda66a-9a38-4394-9231-14bdec8f65d8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Modèle final sauvegardé\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"/content/emotion_faces_v2s_final.keras\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "HDXvyDa9nTUt",
        "outputId": "9a021d55-5bfb-4361-f8e2-d5e94451e167"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7ee290d1-e522-4f9c-ba72-179526699cae\", \"emotion_faces_v2s_final.keras\", 259078759)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}